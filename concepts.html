

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Concepts &mdash; hls4ml 0.5.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Reference and Contributors" href="reference.html" />
    <link rel="prev" title="Command Line Interface" href="command.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/hls4ml_logo_navbar.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="release_notes.html">Realease Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="status.html">Status and Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="command.html">Command Line Interface</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Concepts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-inspiration">The Inspiration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-solution-hls4ml">The Solution: hls4ml</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-it-works">How it Works</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference and Contributors</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/hls-model.html">HLS Model Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/profiling.html">Profiling</a></li>
</ul>
<p class="caption"><span class="caption-text">Autogenerated API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.html">hls4ml</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.converters.html">hls4ml.converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.converters.keras.html">hls4ml.converters.keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.model.html">hls4ml.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.model.optimizer.html">hls4ml.model.optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.model.optimizer.passes.html">hls4ml.model.optimizer.passes</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.report.html">hls4ml.report</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.templates.html">hls4ml.templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.utils.html">hls4ml.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="autodoc/hls4ml.writer.html">hls4ml.writer</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">hls4ml</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Concepts</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="concepts">
<h1>Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline">¶</a></h1>
<p>The goal of <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> is to provide an efficient and fast translation of machine learning models from open-source packages (like Keras and PyTorch) for training machine learning algorithms to high level synthesis (HLS) code that can then be transpiled to run on an FPGA. The resulting HLS project can be then used to produce an IP which can be plugged into more complex designs or be used to create a kernel for CPU co-processing. The user has freedom to define many of the parameters of their algorithm to best suit their needs.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> package enables fast prototyping of a machine learning algorithm implementation in FPGAs,
greatly reducing the time to results and giving the user intuition for how to best design a machine learning algorithm for their application while balancing performance, resource utilization and latency requirements.</p>
<div class="section" id="the-inspiration">
<h2>The Inspiration<a class="headerlink" href="#the-inspiration" title="Permalink to this headline">¶</a></h2>
<p>The inspiration for the creation of the <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> package stems from the high energy physics community at the Large Hadron Collider (LHC). While machine learning has already been proven to be extremely useful in analysis of data from detectors at the LHC, it is typically performed in an “offline” environment after the data is taken and agglomerated. However, one of the largest problems at detectors on the LHC is that collisions, or “events”, generate too much data for everything to be saved. As such, filters called “triggers” are used to determine whether a given event should be kept. Using FPGAs allows for significantly lower latency so machine learning algorithms can essentially be run “live” at the detector level for event selection. As a result, more events with potential signs of new physics can be preserved for analysis.</p>
</div>
<div class="section" id="the-solution-hls4ml">
<h2>The Solution: hls4ml<a class="headerlink" href="#the-solution-hls4ml" title="Permalink to this headline">¶</a></h2>
<img alt="_images/overview.jpg" src="_images/overview.jpg" />
<p>With this in mind, let’s take a look at how <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> helps to achieve such a goal. First, it’s important to realize the architecture differences between an FPGA and a CPU or GPU. An FPGA can be specifically programmed to do a certain task, in this case evaluate neural networks given a set of inputs, and as such can be highly optimized for the task, with tricks like pipelining and parallel evaluation. However, this means dynamic remapping while running isn’t really a possibility. FPGAs also often come at a comparatively low power cost with respect to CPUs and GPUs. This allows <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> to build HLS code from compressed neural networks that results in predictions on the microsecond scale for latency. The <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> tool saves the time investment needed to convert a neural network to a hardware design language or even HLS code, thus allowing for rapid prototyping.</p>
</div>
<div class="section" id="how-it-works">
<h2>How it Works<a class="headerlink" href="#how-it-works" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="_images/nn_map_paper_fig_2.png"><img alt="_images/nn_map_paper_fig_2.png" class="align-center" src="_images/nn_map_paper_fig_2.png" style="width: 70%;" /></a>
<p>Consider a multi-layered neural network. At each neuron in a layer <span class="math notranslate nohighlight">\(m\)</span>  (containing <span class="math notranslate nohighlight">\(N_m\)</span> neurons), we calculate an output value (part of the output vector <span class="math notranslate nohighlight">\(\mathbf{x}_m\)</span> of said layer) using the sum of output values of the previous layer multiplied by independent weights for each of these values and a bias value. An activation function is performed on the result to get the final output value for the neuron. Representing the weights as a <span class="math notranslate nohighlight">\(N_m\)</span> by <span class="math notranslate nohighlight">\(N_{m-1}\)</span>  matrix  <span class="math notranslate nohighlight">\(W_{m,m-1}\)</span>, the bias values as <span class="math notranslate nohighlight">\(\mathbf{b}_m\)</span>, and the activation function as <span class="math notranslate nohighlight">\(g_m\)</span>, we can express this compactly as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}_m = g_m (W_{m,m-1} \mathbf{x}_{m-1} +\mathbf{b}_m)\]</div>
<p>With hls4ml, each layer of output values is calculated independently in sequence, using pipelining to speed up the process by accepting new inputs after an initiation interval. The activations, if nontrivial, are precomputed.</p>
<p>To ensure optimal performance, the user can control aspects of their model, principally:</p>
<ul class="simple">
<li><p><strong>Size/Compression</strong> - Though not explicitly part of the <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> package, this is an important optimization to efficiently use the FPGA resources</p></li>
<li><p><strong>Precision</strong> - Define the <a class="reference internal" href="api/profiling.html"><span class="doc">precision</span></a> of the calculations in your model</p></li>
<li><p><strong>Dataflow/Resource Reuse</strong> - Control parallel or serial model implementations with varying levels of pipelining</p></li>
<li><p><strong>Quantization Aware Training</strong> - Achieve best performance at low precision with tools like QKeras, and benefit automatically during inference with <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> parsing of QKeras models</p></li>
</ul>
<a class="reference internal image-reference" href="_images/reuse_factor_paper_fig_8.png"><img alt="_images/reuse_factor_paper_fig_8.png" class="align-center" src="_images/reuse_factor_paper_fig_8.png" style="width: 70%;" /></a>
<p>Often, these decisions will be hardware dependent to maximize performance. Of note is that simplifying the input network must be done before using <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> to generate HLS code, for optimal compression to provide a sizable speedup. Also important to note is the use of fixed point arithmetic in <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code>. This improves processing speed relative to floating point implementations. The <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> package also offers the functionality of configuring binning and output bit width of the precomputed activation functions as necessary. With respect to parallelization and resource reuse, <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> offers a “reuse factor” parameter that determines the number of times each multiplier is used in order to compute a layer of neuron’s values. Therefore, a reuse factor of one would split the computation so each multiplier had to only perform one multiplication in the computation of the output values of a layer, as shown above. Conversely, a reuse factor of four, in this case, uses a single multiplier four times sequentially. Low reuse factor achieves the lowest latency and highest throughput but uses the most resources, while high reuse factor save resources at the expense of longer latency and lower throughput. The reuse factor can be set using the configuration options defined on the <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a> page.</p>
<p>Thereby, the <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> package builds efficient HLS code to implement neural networks on FPGAs for microsecond-scale latency on predictions. For more detailed information, take a look at our <a class="reference internal" href="reference.html"><span class="doc">References</span></a> page. All figures on this page are taken from the following paper: <a class="reference external" href="https://dx.doi.org/10.1088/1748-0221/13/07/P07027">JINST 13 P07027 (2018)</a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="reference.html" class="btn btn-neutral float-right" title="Reference and Contributors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="command.html" class="btn btn-neutral float-left" title="Command Line Interface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Fast Machine Learning Lab

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>